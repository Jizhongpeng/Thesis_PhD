\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Differences between a traditional camera and a computational camera \cite {nayar2006computational}.}}{43}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Two point representation of the light field. Each ray of light is uniquely defined by the coordinates of two points of interception.}}{45}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Point angle representation of the light field. A ray of light is uniquely defined by the coordinates of a point that belongs to a plane perpendicular at the optical axis z and by the angles $\theta _x$ and $\theta _y$ that it forms with the optical axis along the directions x and y.}}{46}
\contentsline {figure}{\numberline {1.4}{\ignorespaces Plenoptic camera and its fundamental components. The presence of the micro array allows to record the light field .}}{47}
\contentsline {figure}{\numberline {1.5}{\ignorespaces Top: Plenoptic camera 1.0. The main lens is focused on the object and forms an image on the micro array. $f$ is the focal length of the main lens and $f_\mu $ the focal length of the micro lens array. Bottom: Plenoptic camera 2.0. The main lens is focused on an object and forms an image on the plane represented by the dashed line. The micro array acts as a relay between the main lens image and the sensor, satisfying the lens equation $1/a+1/b=1/f_\mu $.}}{48}
\contentsline {figure}{\numberline {1.6}{\ignorespaces Ray diagram of a plenoptic 1.0 system. The main lens is in a 2f configuration and the sensor plane is conjugated with the main lens plane. The micro lens position maps the position (x,y) of the point P, while the sub image maps the directions of the rays coming from that point. The ray with direction $\theta _1$ falls on the pixel 1 (blue), the ray with direction $\theta _2$ falls on the pixel 2 (green) and the ray with direction $\theta _3$ falls on the pixel 3 (red). The sub image $d$ maps the direction of the rays. }}{50}
\contentsline {figure}{\numberline {1.7}{\ignorespaces Sampling of Light Field. Each ray is described by a set of four coordinates, two spatial (x,y) and two directional (u,v). The spatial coordinates are sampled by the position of the lenslet in the micro array, the directional coordinates are sampled by the pixel under the lenslet.}}{52}
\contentsline {figure}{\numberline {1.8}{\ignorespaces F-number matching between the main lens and the micro lens in the array. }}{53}
\contentsline {figure}{\numberline {1.9}{\ignorespaces When the f-number is matched, all the rays captured by the main lens are mapped on the sub image (yellow). If the f-number of the lenslet is smaller (red), the sub image is smaller and there is an under sampling of the set of rays. If the f-number of the lenslet is bigger (green), the sub image is larger and cross talk happens between neighbouring sub images . }}{54}
\contentsline {figure}{\numberline {1.10}{\ignorespaces Sampling of the light field and phase space representations of the rays. }}{56}
\contentsline {figure}{\numberline {1.11}{\ignorespaces Image used as an object in the simulations.}}{58}
\contentsline {figure}{\numberline {1.12}{\ignorespaces Left: Example of a raw plenoptic 1.0 image. Right: Zoom on the raw image of the region indicated with the red square. Each lenslet is formed by 20 $\times $ 20 pixels. Each pixel represents a direction of the rays hitting the lenslet that produced the sub image. }}{58}
\contentsline {figure}{\numberline {1.13}{\ignorespaces The array view is an array of the different point of view obtained rearranging the pixels according to the directional coordinates. }}{59}
\contentsline {figure}{\numberline {1.14}{\ignorespaces Array view of the raw data shown in figure 1.12\hbox {}. }}{61}
\contentsline {figure}{\numberline {1.15}{\ignorespaces Zoom of the central part of the array view showing in detail the different points of views. }}{61}
\contentsline {figure}{\numberline {1.16}{\ignorespaces The intensity of a single pixel is obtained summing all the rays of light coming from all the possible directions.\cite {ng2006digital} }}{62}
\contentsline {figure}{\numberline {1.17}{\ignorespaces Rendering an image from plenoptic 1.0 raw data is equal to summing all the directional samples for each position. This process is shown for the $(x,\theta _x)$ slice of the phase space. \cite {georgiev2010focused} }}{64}
\contentsline {figure}{\numberline {1.18}{\ignorespaces rendered image from the raw data in figure 1.12\hbox {}. Resolution is only 75 $\times $ 75 pixels. }}{65}
\contentsline {figure}{\numberline {1.19}{\ignorespaces If the main lens image is formed in front of the micro array the configuration is called \textit {Copernican}, top, and the focal length of the micro array is smaller then the distance b. If the main lens image is formed behind the micro array the configuration is \textit {Galilean}, on the bottom, and the focal length of the micro array is bigger then the distance b. }}{67}
\contentsline {figure}{\numberline {1.20}{\ignorespaces Raw data of a point source sampled by a plenoptic 2.0 system with a magnification of the micro array stage equal to 0.3. Data acquired with numerical simulations. }}{69}
\contentsline {figure}{\numberline {1.21}{\ignorespaces Zoomed raw data of a point source sampled by a plenoptic 2.0 system with a magnification of the micro array stage equal to 0.3. Data acquired with numerical simulations. The grid represent the boundaries of each sub image to show how the point of view of the point source changes across the lenslets. The sub images are shifted by a quantity proportional to the position of the lenslet in the array. }}{69}
\contentsline {figure}{\numberline {1.22}{\ignorespaces Sampling of the light field by a plenoptic 2.0 system with a magnification of \textit {m=0.3}. For simplicity the one dimensional case is shown. Each micro lens has a diameter equal to d, a focal length $f_{\mu }$, and images the main lens image on the sensor according to the lens law $1/a+1/b=1/f_{\mu }$ The total range of directions that can be sampled is given by $d/b$. Each lenslet samples a sub set of directions equal to $d/a$ as a single direction. The range of directions shown in red are sampled by the central micro lens as a single point of view. The angular resolution is therefore \textit {d/a} and the total number of directional samples is \textit {a/b = 1/m}. In the specific case shown is 3. }}{70}
\contentsline {figure}{\numberline {1.23}{\ignorespaces Raw images of a point source simulated with different magnifications. From top left to bottom right: \textit {m = 1, m = 0.5, m= 0.25, m = 0.1}. }}{72}
\contentsline {figure}{\numberline {1.24}{\ignorespaces System formed by a single lenslet. The rays at the main lens image plane are transformed into the rays at the sensor plane by the lenslet. The transformation can be described by a matrix \textit {A}. }}{75}
\contentsline {figure}{\numberline {1.25}{\ignorespaces Sampling of the light field by plenoptic 2.0 camera. For each of the two points represented, red and green, The total range of directional coordinates are sampled by three different lenslets. For one position three directions are sampled, with a resolution of d/a. Therefore the directional sampling in plenoptic 2.0 is made across many lenslets. This can be seen in the phase space. Lenset B and C sample 2 directions indicated with the same number in the ray diagram and the phase space. Lens let A only samples one direction of the red point. }}{77}
\contentsline {figure}{\numberline {1.26}{\ignorespaces Image rendering with the focused plenoptic camera. One pixel of the rendered image is given by the integration on all the directions $d/b$ associated with a given position. The integration takes place across the lenslet and is represented by the vertical lines. Each lenslet is represented by the diagonal lines. }}{78}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Geometry of the aperture. }}{89}
\contentsline {figure}{\numberline {2.2}{\ignorespaces To remove the scaling factor between the input and output fields, a multi step Fresnel approach has been developed. The field is propagated by unit of dz, the minimum distance to keep the sampling the same. }}{92}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Structure of the operator free space propagation with the angular spectrum of plane waves method. The initial disturbance $U(x,y;0)$ is transformed into the angular spectrum $A(f_x,f_y;0)$ with a Fourier transform implemented by a FFT algorithm. The angular spectrum is multiplied by the propagation transfer function $H(f_x,f_y)$ and the resultant angular spectrum is inverse transformed into the output disturbance $U(x,y;z)$ }}{98}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Cross section along of the phase of the transfer function of the angular spectrum. It is evident how increasing the propagation distance increases the oscillating frequency leading to aliasing. }}{99}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Bandwidth of the transfer function for three different propagation distances. }}{102}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of the operator free space propagation with the angular spectrum of plane waves method in its band limited version. The initial disturbance $U(x,y;0)$ is transformed into the angular spectrum $A(f_x,f_y;0)$ with a Fourier transform implemented by a FFT algorithm. The angular spectrum is multiplied by the propagation transfer function $H(f_x,f_y)$ whose bandwidth has been limited according to equation 2.63\hbox {}. The bandwidth of the transfer function depends on the sampling of the input field, the wavelength of the light $\lambda $ and the propagation distance.The resultant angular spectrum is inverse transformed into the output disturbance $U(x,y;z)$. \textit {N} is the sampling of thr input field, \textit {z} the propagation distance and $\lambda $ the wavelength. }}{104}
\contentsline {figure}{\numberline {2.7}{\ignorespaces The maximum spatial frequency is linked to the dimension of the sampling window of the output field $w$ and the propagation distance $z$ }}{106}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Comparison between the Signal to Noise ratio (SNR) as a function of the propagation distance for the three propagation methods seen in section 2.4.1\hbox {}. The BL and the corrected BL methods improve the SNR, that instead drops as the propagation distance increases when the AS method is used. }}{108}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Comparison of the performances of the three angular spectrum propagation operator. On the left column are shown diffraction pattern cross section obtained using the angular spectrum method (AS). In the central column the pattern for the same propagation distances have been obtained using the band limited method (BL) and in the right column the same patterns have been obtained using the correct band limited method (corrected BL). }}{109}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Thickness of a lens as a function of position along x coordinate. It can be defined in the same way along the y coordinate}}{110}
\contentsline {figure}{\numberline {2.11}{\ignorespaces The thickness function can be decomposed into the sum of three contributions. }}{112}
\contentsline {figure}{\numberline {2.12}{\ignorespaces A thin lens converts a plane wave into a spherical wave. }}{114}
\contentsline {figure}{\numberline {2.13}{\ignorespaces Phase profile of a computer generated lens. The blue line shows the quadratic phase term and the red line shows the same phase term wrapped every $2\pi $. This wrapping is the source of the aliasing. }}{115}
\contentsline {figure}{\numberline {2.14}{\ignorespaces Schematic of the 2f system simulated. f is the focal lenght, z the propagation distances, D is the aperture of the lens, w the field of view and N the sampling resolution. }}{119}
\contentsline {figure}{\numberline {2.15}{\ignorespaces Operator sequence for the 2f system. }}{120}
\contentsline {figure}{\numberline {2.16}{\ignorespaces From top to Bottom: Image and intensity cross section of a point source according to multi step method, angular spectrum method, band limited angular spectrum method and corrected band limited angular spectrum method. }}{121}
\contentsline {figure}{\numberline {2.17}{\ignorespaces From top left to bottom right: Power spectrum of the image of a point source obtained with the MSF method, the AS method, the BL method and the CBL method . }}{123}
\contentsline {figure}{\numberline {2.18}{\ignorespaces Young experiment setup.}}{126}
\contentsline {figure}{\numberline {2.19}{\ignorespaces A random phase mask is generated by propagating light coming form an array of sources, each one with a random phase value in the interval $[-\pi ,\pi ]$. }}{128}
\contentsline {figure}{\numberline {2.20}{\ignorespaces Process of the creation of the phase mask. Top: array of point sources with a random phase value; centre: array of the areas of coherence at source after the convolution with $K$; bottom: randomized areas of coherence at object plane after the propagation of $d_c$. }}{130}
\contentsline {figure}{\numberline {2.21}{\ignorespaces Final random phase mask generated a coherence index C equals to 10, 50, 100 and 200. }}{131}
\contentsline {figure}{\numberline {2.22}{\ignorespaces Emission spectrum of the LED considered in our simulations and real image system. Data plotted from Thorlabs LED Array Light Source LIU630A datasheet. }}{133}
\contentsline {figure}{\numberline {2.23}{\ignorespaces USAF resolution target. }}{135}
\contentsline {figure}{\numberline {2.24}{\ignorespaces Images of the USAF resolution target with different values of $\iota $. Increasing the number of point sources generating the phase mask leads to more incoherent imaging, leading to improved resolution. Images where obtained with 300 iterations.}}{136}
\contentsline {figure}{\numberline {2.25}{\ignorespaces Signal to noise ratio of the image of a USAF resolution target plotted as a function of the coherence index $C$.}}{137}
\contentsline {figure}{\numberline {2.26}{\ignorespaces Noise due to the speckles in an image of a USAF resolution target after only 5 iterations.}}{138}
\contentsline {figure}{\numberline {2.27}{\ignorespaces Images of the USAF resolution target obtained adding an increasing number of snapshot. this is equivalent to increase the integration time of the sensor. The noise due to the speckles caused by the phase mask decreases with increasing number of iterations.}}{139}
\contentsline {figure}{\numberline {2.28}{\ignorespaces Signal to noise ratio as a function of the number of iterations. The threshold has been calculated looking at the variance of the previous five data.}}{140}
\contentsline {figure}{\numberline {2.29}{\ignorespaces Comparison between the coherent and an incoherent response of a simple 2f system to a sharp edge.}}{142}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Parameters that characterize the micro lens array are: the pitch $p$, defined as the distance between the centres of two lenslets, the diameter d, the number of lenslets per row N and the size of the micro array W. }}{145}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Parameters that characterize the micro lens array. }}{145}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Values of acceptable lenslet diameter to avoid diffraction induced cross talk. }}{147}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Raw plenoptic image of a point source. The grid represents the edges of the sub images. If the point source is in focus, it should be represented by only one lenslet. Because of diffraction some light goes also on the neighbouring sub images. }}{148}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Phase space of the light field of a point object: on the left is the case when the condition 3.3\hbox {} is respected; on the right when diffraction induces cross talk that arises between neighbouring lenslets. Cross talk introduces a blur in the rendered image whose width is $\Delta x$. }}{148}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Rendering seen from the $(x,\theta _x)$ slice of the phase space. The directional pixels not considered for the integration are shown in grey. }}{151}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Raw plenoptic 1.0 image obatined from Levoy \textit {et al.} The microscope was a Zeiss Axiovert with a 20x/0.5NA (dry) objective. The micro lens array was 24mm x 36mm, with square 125-micron x 125-micron f/20 micro lenses, held in front of the Axiovert's side camera port using an optical bench mount. The camera was a Canon 5D full-frame digital SLR. The specimen was the thin silky skin separating two layers of an onion, immersed in oil to improve transparency \cite {levoy2006microscope}. }}{152}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Rendered Images from the raw data in figure 3.7\hbox {}. On the left there is the full aperture image obtained integrating along all the directional coordinates, while on the right there is the all in focus image obtained taking only the central pixel of the each sub image, that corresponds to rays with the direction parallel to the optical axis.}}{152}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Sampling of the light field of a point source in focus, on the top, closer to the main lens, centre, and further away, bottom. When the source is out of focus, both position and directions are sampled by more than one lenslet since the main lens image is no longer formed any more on the lenslet plane. }}{155}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Information on the depth of a point source imaged by a plenoptic 1.0 system. As explained in section 1.4.1\hbox {}. If the point source is in focus, on the top, its position is sampled by one lenslet as well as the whole set of directions.}}{156}
\contentsline {figure}{\numberline {3.11}{\ignorespaces Physical meaning of the slope in the phase space. If the point source is further then the camera focal plane, the phase space line has a negative slope. If the point source is closer then the focal plane, the slope of the phase space line is positive.}}{157}
\contentsline {figure}{\numberline {3.12}{\ignorespaces Numerical simulation of a point source imaged by a plenoptic 1.0 imaging system. The point source was placed at three different distances from the main lens. The top three figures show the focused image. In the centre ones a defocus of 0.125 m closer to the main lens is added. In the bottom ones the point source is closer to the main lens of 0.25 m. In all the three cases the raw data image is on the left, the rendered image is on the centre and the phase space line is on the right.}}{158}
\contentsline {figure}{\numberline {3.13}{\ignorespaces The synthetic camera refocusing method is based on the fact that is always possible to define a virtual aperture that is focused on the lenslet plane. }}{160}
\contentsline {figure}{\numberline {3.14}{\ignorespaces Changing the focal plane of the camera is equal to reparametrizing the light field according to the new coordinates. The reparametrization corresponds to a scaling of the ratio between the refocused plane and the original camera plane $ \alpha '$. }}{162}
\contentsline {figure}{\numberline {3.15}{\ignorespaces Changing the position of the focal plane is equal to shear the light field in the phase space. The total area remains the same since the total light field is conserved. }}{165}
\contentsline {figure}{\numberline {3.16}{\ignorespaces Flow chart of the operator shearing. }}{166}
\contentsline {figure}{\numberline {3.17}{\ignorespaces Set up of a plenoptic 1.0 camera replicated in the simulations. }}{167}
\contentsline {figure}{\numberline {3.18}{\ignorespaces Position of the point sources imaged inside the volume V simulated. }}{168}
\contentsline {figure}{\numberline {3.19}{\ignorespaces Raw image on the left and rendered image on the right. Intensity is shown in false colour in order to appreciate variations in energy distribution. }}{169}
\contentsline {figure}{\numberline {3.20}{\ignorespaces Values of the refocusing factor $ \alpha '$ as a function of the defocus. A positive defocus means that the object it further away with respect to the main lens, a negative defocus indicates an object that is closer. }}{170}
\contentsline {figure}{\numberline {3.21}{\ignorespaces Refocused images of the three point sources in figure 3.19\hbox {}. In the top one the point A is refocused, in the bottom one the point B is refocused. When the point source is refocused, the intensity distribution is higher with respect to the out of focus points. }}{171}
\contentsline {figure}{\numberline {3.22}{\ignorespaces Spectrum of out of focus image (blue) and spectrum of the refocused image (red). }}{173}
\contentsline {figure}{\numberline {3.23}{\ignorespaces Energy contained in the spectrum of the rendered image plotted as a function of the refocusing factor $ \alpha '$. In red is indicated the value of $ \alpha '$ corresponding to the image with the maximum energy. }}{175}
\contentsline {figure}{\numberline {3.24}{\ignorespaces Estimated values of $ \alpha '$ represented by blue stars and the theoretical values represented with the red line. }}{176}
\contentsline {figure}{\numberline {3.25}{\ignorespaces Distance between the estimated values of $ \alpha '$ and the actual theoretical values. }}{176}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Top: the point $A$ is imaged by the main lens into the point $A'$. It is imaged by the lenslet array into a number of sub images depending by the magnification of the micro array stage, in this case \textit {m=0.333}. Bottom: particular of the raw image of the point source $A$ and phase space diagram. The three sub images \textit {1, 2} and \textit {3} are three different points of view of the object. }}{181}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Relation between the position of the centre of a lenslet \textit {x} and the position of the centre of the correspondent sub image $x + x'$. }}{183}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Basic rendering algorithm. The Rendered image is made by tiling together patches extracted by each sub image. The patch size is dependent on the magnification of the micro array stage. }}{184}
\contentsline {figure}{\numberline {4.4}{\ignorespaces The points that belong to out of focus object plane represented in red are imaged on a virtual main lens image plane. This plane is relayed on a virtual sensor plane by a virtual micro array whose parameters are \textit {a'} and \textit {b'} and \textit {m' = b'/a'}. }}{186}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Values of the magnification used to determine the patch size for the plenoptic 2.0 rendering plotted as a function of the defocus. }}{187}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Ray diagram of the system simulated. }}{188}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Values of the parameter \textit {b} correspondent to values of magnification varying from 0.1 to 0.5.}}{190}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Values of the parameter \textit {a} corresponding to values of magnification varying from 0.1 to 0.5.}}{190}
\contentsline {figure}{\numberline {4.9}{\ignorespaces Aperture radius of the main lens as a function of the magnification in order to respect the f-number matching condition.}}{191}
\contentsline {figure}{\numberline {4.10}{\ignorespaces The extension of the central lobe of the Airy disk is defines the lateral resolution of optical system. Intensity profile acquired using the parameters listed in table 4.1\hbox {}}}{194}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Lateral resolution as a function of the magnification.}}{196}
\contentsline {figure}{\numberline {4.12}{\ignorespaces Optical cut-off frequency as a function of the magnification.}}{196}
\contentsline {figure}{\numberline {4.13}{\ignorespaces Left: raw sensor image of a point source, Right: rendered image. Artifacts are present. Magnification of the lenslet array: 0.5 }}{198}
\contentsline {figure}{\numberline {4.14}{\ignorespaces Left: raw sensor image of a point source, Right: rendered image. Artifacts are present. Magnification of the lenslet array: 0.3.}}{198}
\contentsline {figure}{\numberline {4.15}{\ignorespaces Left: raw sensor image of a point source, Right: rendered image. Artefacts are present. Magnification of the lenslet array: 0.25 }}{199}
\contentsline {figure}{\numberline {4.16}{\ignorespaces From top to bottom: comparison between the main lens image profile, in blue, and the rendered image profile, in red, for magnification values of 0.5, 0.3 and 0.25. The central lobe of the rendered image increase with the magnification, as well as the number of artefacts. }}{201}
\contentsline {figure}{\numberline {4.17}{\ignorespaces On the top left there is the main lens image of two point sources separated by the Rayleigh distance $\delta x$. On the top right its corresponding rendered plenoptic image obtained with a magnification of 0.5. As expected from the theory, the resolution of the rendered image is half the one af the main lens image, and the two points are no more distinguishable as confirmed by the intensity profiles of the two images on the bottom. }}{203}
\contentsline {figure}{\numberline {4.18}{\ignorespaces The image on the top left shows the main lens image of two point sources separated by the Rayleigh distance $2\delta x$. On the top right there is its corresponding rendered plenoptic image obtained with a magnification of 0.5. Doubling the Rayleigh distance the rendered image is at the limit of resolution, where the maximum of the Airy pattern of one point falls on the first zero of the second point. }}{204}
\contentsline {figure}{\numberline {4.19}{\ignorespaces Modulation transfer functions of the main lens (green) and the overall imaging system (red) for a magnification value of $m = 0.5$ , top, $m = 0.3$, central, and $m = 0.25$, bottom.}}{207}
\contentsline {figure}{\numberline {4.20}{\ignorespaces The object imaged in this simulation is a sinusoidal grating containing five different frequencies. }}{209}
\contentsline {figure}{\numberline {4.21}{\ignorespaces Top: on the left is the raw sensor image, on the right the main lens image. Bottom: On the left there is the rendered image, on the right the same image has been low pass filtered to remove some of the rendering artefacts. Magnification was 0.5 }}{210}
\contentsline {figure}{\numberline {4.22}{\ignorespaces On the left: intensity profiles of the main lens image, top and of the rendered image,bottom. On the right the spatial frequencies modulated according to the MTF as shown by the power spectra of the main lens image on the top right and of the rendered image on the bottom right. The magnification of the lenslet array is \textit {m = 0,5} and the first zero frequency of the system is $f_{cut-off} = 10^4 m{-1}$}}{211}
\contentsline {figure}{\numberline {4.23}{\ignorespaces Evolution of the spectrum of the optical field as it propagates through the imaging system. First step the field is propagated through the main lens and its frequency content is modulated by its MTF whose bandwidth is $\Delta \nu _1$. Then modulated field passes through the lenslet array stage and because of the magnification that is less than one its bandwidth is reduced further to $\Delta \nu _2$. Since $\nu _1 > \nu _2$ the imaging system is modelled as a linear low pass filter with a bandwidth $\Delta \nu _1$ defined by the magnification of the micro array stage. }}{213}
\contentsline {figure}{\numberline {4.24}{\ignorespaces Top: on the left it is shown the raw sensor image, on the right the main lens image. Bottom: On the left there is the rendered image, on the right the same image has been low pass filtered to remove some of the rendering artefacts. Magnification was 0.3. }}{214}
\contentsline {figure}{\numberline {4.25}{\ignorespaces On the left: intensity profiles of the main lens image, top and of the rendered image,bottom.On the right: spatial frequency modulation done by MTF. The first zero frequency of the system is $0.756 \times 10^4 m{-1}$}}{215}
\contentsline {figure}{\numberline {4.26}{\ignorespaces Aliased frequencies transmitted to the rendered image. }}{216}
\contentsline {figure}{\numberline {4.27}{\ignorespaces Top: raw sensor image, on the right the main lens image. Bottom: On the left there is the rendered image, on the right the same image has been low pass filtered to remove some of the rendering artefacts. Magnification was 0.3. }}{217}
\contentsline {figure}{\numberline {4.28}{\ignorespaces On the left: intensity profiles of the main lens image, top and of the rendered image,bottom.On the right there are the spatial frequencies modulated according to the MTF. The first zero frequency of the system is $0.6 \times 10^4 1/m$}}{218}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Schematic diagram of the light field microscope. The values of the distances are: $z_c$=27 mm, $z_k$=120 mm, $z_1$=$z_2$=120 mm. }}{225}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Schematics of the light field recording stage. The first micro metric translator moves both the sensor and micro array stages. The second only moves the micro array stage with respect to the sensor stage. }}{226}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Assembled light field recording stage with. }}{226}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Lenslet array parameters.}}{227}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Common causes of bad alignment between the micro lens array and the sensor. }}{231}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Calibration image with f-number not matched. }}{232}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Calibration image with matched f-number. }}{232}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Raw plenoptic 2.0 image of the USAF resolution target obtained by the prototype system built in lab. The top image has a magnification of 0.5, the bottom 0.4. }}{233}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Raw plenoptic 2.0 image of the USAF resolution target obtained by the prototype system built in lab. The top image has a magnification of 0.3, the bottom 0.25. }}{234}
\contentsline {figure}{\numberline {5.10}{\ignorespaces From the calibration image a row and a column of sub images are selected by the user with the red lines on the top if the figure. Two intensity cross sections are generated (un the bottom) and the user manually sets a threshold to discriminate one sub image from the other.}}{237}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Rendered images of a 1951 USAF resolution target. The correspondent raw data are in figures 5.8\hbox {} and 5.9\hbox {}. Magnifications are from top left to bottom right: \textit {0.25 0.3 0.4} and \textit {0.5}. }}{238}
\contentsline {figure}{\numberline {5.12}{\ignorespaces Top: Rendered image obtained from the same raw data used to render the image on the bottom right in figure 5.11\hbox {}. In this case the grid used to select the micro images is not correctly set, presenting an offset of 15 pixel for every sub image and errors in detecting the sub image as the bottom figure shows.}}{239}
\addvspace {10\p@ }

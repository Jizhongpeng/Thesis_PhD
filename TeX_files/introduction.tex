\chapter*{Introduction}
\markboth{\MakeUppercase{Introduction}}{}
\addcontentsline{toc}{chapter}{Introduction}
\label{intro}
The information contained in the light surrounding the environment we live in is captured by different living beings according to the structure and the nature of their visual systems. The human eye is a single lens imaging system whose retina is composed of three different types of photosensitive cells \cite{hart1992adler}. For this reasons cameras have always been designed to capture light in the same way the human eye works, a single lens with three colour pixels producing two dimensional images based on three colours \cite{wetzstein2011computational}. \\
This way of designing cameras produces a loss of information with respect to all the information included into the light reaching the imaging system.\\
 The reality we see and measure with the instruments and the senses we have is just a projection of a more complex reality and great part of the information carried by the electromagnetic field is lost. This situation is similar to the one described in Plato's Allegory of the cave where a group of people are being chained for all their life inside a cave and facing a blank wall on which shadows of different objects are projected. For them the shadows produced by the objects passing in front of the fire are the only reality they know since they are limited in their perception by their situation \cite{plato1945republic} . The same happens with human perception and with the camera that imitates the human visual system. All conventional imaging instruments are limited and the images they produce are projections of a more complex quantity containing all information that light carries. Even the most advanced imaging instrument will always produce data that are close to reality, but that does not reach it, both because of intrinsic properties of light, such as diffraction, and because they have been designed to collect only a small set of reality. 
 Adelson and Bergen \cite{adelson1991plenoptic} in 1991 proposed to describe the whole set of information contained in the light by defining the plenoptic function, a seven dimensional function describing intensity, direction and wavelength of rays propagating in the space. According to Adelson and Bergen a conventional imaging device captures projections of this function. For example a photo is obtained by integrating along the directional variables of the function, obtaining a two dimensional intensity pattern of the object imaged losing information on its three dimensional structure. If the integration is performed also along wavelength the image will be in grayscale and the information on the wavelength will be lost. \cite{adelson1992single,ng2006digital}.
\\
This work will describe a new set of instruments that are able to sample the plenoptic function and the post processing methods to access all the information it carries. The character of this thesis will be mostly computational, and numerical simulations will be used as an instrument to explore the potential and define the limitations of this class of instruments.
\\
 The idea that more information is carried by the light contained in a scene was first proposed by Leonardo da Vinci who described the volume containing the light as filled by a dense array of light rays, which he called \textit{radiant pyramids} \cite{adelson1991plenoptic}. The plenoptic function described by Adelson and Wang \cite{adelson1991plenoptic} is the formalization of this concept of \textit{radiant pyramids} adding to the directions and positions of the rays information on the wavelength. Once the extra information contained in the scene has been defined the problem is to find a method to measure it. \\
 The first work about capturing the plenoptic function was done by Frederic Yves in 1903 \cite{ives1903parallax} on his works on parallax panorama-grams and by Lippmann in 1908 \cite{gabriel1908photographie} who proposed an array of pinhole cameras. The key to capture the plenoptic function is to have multiple views of the object to image. This can be done by translating a single camera to different positions over a spherical or planar surface \cite{levoy1996light,taguchi2010axial,gortler1996lumigraph}, method known as time sequential sampling, or by using an array of cameras \cite{wilburn2001light,yang2002real,nomura2007scene} known as multi-sensor sampling. Multi-sensor techniques give high performances in term of resolution but are extremely complicated and expensive, while time-sequential techniques suffer the loss of the capability to capture dynamic scenes \cite{wetzstein2011computational}. The solution between cost and resolution is given by single-shot plenoptic sampling techniques based on a space multiplexing of the plenoptic function \cite{wetzstein2011computational}, as proposed by Herbert Ives \cite{ives1930parallax} and Lippmann \cite{gabriel1908photographie}. This single-shot space multiplexing method produces an array of elemental images on the sensor and this introduces several trade-offs between spatial and directional resolution \cite{wetzstein2011computational,georgiev2006spatio} as well optical resolution trade-offs \cite{turola2014wave}. Several families of devices have been developed.\\ This work will describe those that sample the plenoptic function using an array of micro lenses in front of the sensor as described by Adelson and Wang \cite{adelson1992single}, Ng \cite{ng2005light}, Ng \textit{et al.}\cite{ng2006digital}, Ueda \textit{et al.} \cite{ueda2008adaptive} and Georgiev and Lumsdaine \cite{georgiev2010focused}. In particular Ng \textit{et al.} developed the first generation of plenoptic imaging systems, known as plenoptic 1.0, then improved by Georgiev who modified them giving birth to the second generations, or plenoptic 2.0. These two classes of instruments will be treated in this work. Other methods to capture the plenoptic function are based on creating mask patterns that enable frequency domain multiplexing of the plenoptic function \cite{veeraraghavan2007dappled,lanman2008shield}, compressive imaging techniques \cite{marwah2013compressive} or exotic solutions such as a slit camera \cite{ji2015depth}. The resulting images obtained with all these different methods contains all the extra information contained in the plenoptic function, but they are codified. Therefore the final image is obtained after implementing post processing algorithms in order to decode this information. For this reason the process of capturing plenoptic images is also called computational photography \cite{wetzstein2011computational,nayar2006computational}.\\ 
Along with the image rendering several post processing features can be performed once the information captured is decoded. These features include digital refocus and changing the point of view \cite{ng2006digital}, estimating depth \cite{ji2015depth,wetzstein2011computational,georgiev2012multifocus,georgiev2006light} and super-resolution imaging \cite{bishop2009light,shroff2013image,georgiev2009superresolution,georgiev2015plenoptic}. \\
\subsection*{Summary of the Thesis and its Novel Contribution to the Field}
The purpose of this work is to analyse the optical performances of micro lens array based plenoptic imaging systems. When this project started all existing work on this kind of instrument had been done using a ray optics approach. This work uses a wave optics approach so as to study the behaviour of a plenoptic imaging system at its diffraction limit. This is the first work that quantifies the effects of diffraction on the quality of raw sensor images and rendered images, defining a new trade-off between spatio-angular resolution and optical resolution. Most of the applications of plenoptic imaging considered in the literature preceding this work regarded photography and imaging of macroscopic objects. When the attention is moved to the microscopic world, diffraction effects become important. The first thing that has been done was to develop a simulation platform for wave propagation in a generic optical system. Existing methods to implement free space propagation have been investigated and a variant of one of the existing methods has been proposed in order to make it more efficient. This work also defines a new empirical method to simulate light in any state of coherence based on the true physical nature of coherence and it has been shown to give realistic results. Once the simulation platform has been properly tested it has been used to simulate a plenoptic imaging system. Different parameters have been changed in order to investigate their influence on the optical properties of the system. At the same time post processing algorithms to extract information from the raw sensor data have been developed following the guidelines present in literature. Some innovative solutions have also been applied. Different configurations of plenoptic setups have been compared, with particular attention to the optical performances of plenoptic 2.0 imaging systems. The results from these studies were then used to build a plenoptic microscope prototype in the laboratory.
\\
In the first chapter an introduction of plenoptic imaging will be given. A rigorous explanation of how the seven dimensional plenoptic function can be reduced to a four dimensional function called light field is described. A definition of the light field will be given and an explanation of how the light field carries information regarding the 3D structure of the scene will be provided including how it is sampled by the two different configurations, plenoptic 1.0 and plenoptic 2.0. A rigorous theoretical description of both classes of plenoptic systems will be given using geometrical optics, with particular attention to the post processing methods to decode the information in it. \\
The second chapter is a description of the simulation platform developed in MATLAB that implements wave propagation of light under the Fresnel approximation. It has been designed to be a versatile and modular platform composed of operators that act on an input optical field to give an output field. The first operator to be described is the free space propagation based on the Fresnel approximation. The scalar theory of diffraction will be treated from a theoretical point of view starting from Maxwell's equations to end with the expression of Fresnel diffraction integral. Different methods to implement Fresnel propagation will be analysed highlighting advantages and disadvantages of each one of them in terms of resolution, signal to noise ratio and computational effort. To optimize the trade-off between the signal to noise ratio and digital aliasing a new wave propagation method was developed in this work and will be presented in this chapter. Then the lens operator will be described and used to show results of simulation of simple optical systems with the purpose of comparing the performances of the five different free space propagation operators. This chapter is concluded with an explanation of the method to simulate partially coherent light which was developed.
\\
The third chapter is a complete description of a plenoptic 1.0 system explaining how to design and run simulations and how the to implement existing post processing algorithms in MATLAB to render the final image from the raw sensor data. Digital refocus principles and methods will be described. An original method to estimate the depth of point sources will be also proposed. 
\\
Chapter four focusses on the simulations performed to define the optical properties of a plenoptic 2.0 system. The impulse response of the system will be calculated together with its modulation transfer function. A frequency analysis of the system will be presented, showing how the different parameters of the system influence the optical resolution of the final rendered image. The detailed optical analysis of a plenoptic system using wave optics approach is one of the key novel contributions of this work to the field.
\\
Chapter five contains the description of the setup built in the laboratory to verify the conclusions achieved with the simulations. A full description of the system will be given, as well as a description of the protocol to follow to capture light field images. Results from this laboratory prototype will be presented.
\\

